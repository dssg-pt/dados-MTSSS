{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import dateparser\n",
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nº de Baixas Por Isolamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixas_all_data():\n",
    "    df_baixas = pd.read_csv('../original_files/Baixas por Isolamento.csv')\n",
    "    #Select the relevant information, deleting the first 8 rows\n",
    "    df_baixas = df_baixas.iloc[6:]\n",
    "    baixas = df_baixas[df_baixas.columns[:3]]\n",
    "    #Reset indexes\n",
    "    baixas.reset_index(drop=True, inplace=True)\n",
    "    #Get indexes that separate tables\n",
    "    b = np.concatenate(( [True], pd.isnull(baixas.iloc[:,0]), [True] )) \n",
    "    indexes = np.flatnonzero(b[1:] != b[:-1]) \n",
    "    baixas_all = baixas[indexes[0]:indexes[1]]\n",
    "    #dropping rows with nan values\n",
    "    baixas_all.dropna(inplace=True)\n",
    "    #renaming the columns\n",
    "    baixas_all.columns = ['DATA', 'POR DIA', 'ACUMULADOS']\n",
    "    #fomatting the date \n",
    "    baixas_all['DATA'] = baixas_all['DATA'].apply(dateparser.parse)\n",
    "    baixas_all.to_csv(r'../dataframes/baixas_all.csv', index = False)\n",
    "    return baixas_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Ana\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>POR DIA</th>\n",
       "      <th>ACUMULADOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2020-07-03</td>\n",
       "      <td>461</td>\n",
       "      <td>136144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>21</td>\n",
       "      <td>136165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2020-07-05</td>\n",
       "      <td>8</td>\n",
       "      <td>136173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>486</td>\n",
       "      <td>136659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>222</td>\n",
       "      <td>136881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DATA POR DIA ACUMULADOS\n",
       "124 2020-07-03     461     136144\n",
       "125 2020-07-04      21     136165\n",
       "126 2020-07-05       8     136173\n",
       "127 2020-07-06     486     136659\n",
       "128 2020-07-07     222     136881"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baixas_all_data().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixas_distrito_data():\n",
    "    df_baixas = pd.read_csv('../original_files/Baixas por Isolamento.csv')\n",
    "    #Select the relevant information, deleting the first 8 rows\n",
    "    df_baixas = df_baixas.iloc[6:]\n",
    "    baixas = df_baixas[df_baixas.columns[:3]]\n",
    "    #Reset indexes\n",
    "    baixas.reset_index(drop=True, inplace=True)\n",
    "    #Get indexes that separate tables\n",
    "    b = np.concatenate(( [True], pd.isnull(baixas.iloc[:,0]), [True] )) \n",
    "    indexes = np.flatnonzero(b[1:] != b[:-1]) \n",
    "    baixas_distrito = baixas[indexes[4]:indexes[5]]\n",
    "    #dropping rows with nan values\n",
    "    baixas_distrito.dropna(axis=1, how='any', inplace=True)\n",
    "    #renaming the columnsS']\n",
    "    baixas_distrito.columns = ['DISTRITO', 'TOTAL']\n",
    "    baixas_distrito.to_csv(r'../dataframes/baixas_distrito.csv', index = False)\n",
    "    return baixas_distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISTRITO</th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>136881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>AVEIRO</td>\n",
       "      <td>69214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>BEJA</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>BRAGA</td>\n",
       "      <td>7998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>BRAGANÇA</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     DISTRITO   TOTAL\n",
       "135     TOTAL  136881\n",
       "136    AVEIRO   69214\n",
       "137      BEJA     480\n",
       "138     BRAGA    7998\n",
       "139  BRAGANÇA     535"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baixas_distrito_data().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lay Off - Estimativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layoff_data():\n",
    "    df_layoff = pd.read_csv('../original_files/Layoff – Estimativa .csv')\n",
    "    df_layoff.dropna(axis=0, how='any', inplace=True)\n",
    "    df_layoff.columns = ['DATA', 'Nº NISS_EE', 'Nº TRABALHADORES', \n",
    "                            'REMUNERAÇÕES DECLARADAS']\n",
    "    df_layoff.drop(['Nº NISS_EE'], axis=1, inplace=True) \n",
    "    df_layoff['DATA'] = df_layoff['DATA'].apply(lambda x: x.split(' ')[0])\n",
    "    df_layoff['DATA'] = df_layoff['DATA'].apply(lambda x: x.replace('/', '-'))\n",
    "    df_layoff['DATA'] = df_layoff['DATA'].apply(lambda x: x.split('-'))\n",
    "    df_layoff['month'] = df_layoff['DATA'].apply(lambda x: x[1])\n",
    "    df_layoff['day'] = df_layoff['DATA'].apply(lambda x: x[2] if len(x[0]) > 2 else x[0])\n",
    "    df_layoff['year'] = df_layoff['DATA'].apply(lambda x: x[0] if len(x[0]) > 2 else x[2])\n",
    "    df_layoff['DATA'] = pd.to_datetime(df_layoff[['day', 'month', 'year']])\n",
    "    df_layoff.drop(['month', 'day', 'year'], axis=1, inplace=True) \n",
    "    df_layoff['DATA'] = df_layoff['DATA'].mask(df_layoff['DATA'].dt.year == 2021, \n",
    "                             df_layoff['DATA'] + pd.offsets.DateOffset(year=2020))\n",
    "    df_layoff.to_csv(r'../dataframes/df_layoff.csv', index = False)\n",
    "    return df_layoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:692: PerformanceWarning: Non-vectorized DateOffset being applied to Series or DatetimeIndex\n",
      "  PerformanceWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>Nº TRABALHADORES</th>\n",
       "      <th>REMUNERAÇÕES DECLARADAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>72507</td>\n",
       "      <td>69174523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>231683</td>\n",
       "      <td>225500911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>350028</td>\n",
       "      <td>339900463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>425287</td>\n",
       "      <td>434790415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>551955</td>\n",
       "      <td>570960157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATA Nº TRABALHADORES REMUNERAÇÕES DECLARADAS\n",
       "6  2020-03-31            72507                69174523\n",
       "7  2020-04-01           231683               225500911\n",
       "8  2020-04-02           350028               339900463\n",
       "9  2020-04-03           425287               434790415\n",
       "10 2020-04-04           551955               570960157"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layoff_data().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layoff – Estim. - CAE,Dim,Dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layoff por setor económico- Quantidade de companhias em layoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historic_layoff_CompaniesAmount_bySector(): \n",
    "    data1 = pd.read_csv('../original_files/historical_data_company.csv')\n",
    "    data2 = pd.read_csv('../original_files/Layoff – Estim. - CAE,Dim,Dist.csv')\n",
    "    #Select the relevant information, deleting the first 8 rows\n",
    "    df=data2.iloc[8:]\n",
    "    #import today´s date\n",
    "    today=date.today()\n",
    "    #Select the columns that are relevant to work sectors\n",
    "    df_work=df[df.columns[0:3]]\n",
    "    df_work=df_work.iloc[2:23]\n",
    "    #Rename columns for data cleaning\n",
    "    df_work=df_work.rename(columns={'EEs QUE ENTREGARAM DOCUMENTO - COVID19 - Layoff Simplificado':'Index','Unnamed: 1':'Setor',\n",
    "                       'Unnamed: 2':today,'Unnamed: 3':'Nº TRABALHADORES','Unnamed: 4':'Feminino','Unnamed: 5':'Masculino'})\n",
    "    #Drop columns for data cleaning\n",
    "    df_work=df_work.drop(['Index'],axis=1)\n",
    "    data1=data1.drop(['Unnamed: 0'],axis=1)\n",
    "    #Merge historical data with last report\n",
    "    new_historical=pd.merge(left=data1, right=df_work, left_on='Setor', right_on='Setor')\n",
    "    new_historical=new_historical.sort_values(by=[today],ascending=False)\n",
    "    #Save new data as historical data\n",
    "    new_historical.to_csv('../original_files/historical_data_company.csv')\n",
    "    return new_historical               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layoff por setor económico- Quantidade de trabalhadores em layoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historic_layoff_PeopleAmount_bySector():\n",
    "    data3 = pd.read_csv('../original_files/historical_data_person.csv')\n",
    "    data4 = pd.read_csv('../original_files/Layoff – Estim. - CAE,Dim,Dist.csv')\n",
    "    #Select the relevant information, deleting the first 8 rows\n",
    "    df=data4.iloc[8:]\n",
    "    #import today´s date\n",
    "    today=date.today()\n",
    "    #Select the columns that are relevant to work sectors\n",
    "    df_work=df[df.columns[0:4]]\n",
    "    df_work=df_work.iloc[2:23]\n",
    "    #Rename columns\n",
    "    df_work=df_work.rename(columns={'EEs QUE ENTREGARAM DOCUMENTO - COVID19 - Layoff Simplificado':'Index','Unnamed: 1':'Setor',\n",
    "                       'Unnamed: 2':'Nº NISS_EE','Unnamed: 3':today,'Unnamed: 4':'Feminino','Unnamed: 5':'Masculino'})\n",
    "    #Drop columns for data cleaning\n",
    "    df_work=df_work.drop(['Index','Nº NISS_EE'],axis=1)\n",
    "    data3=data3.drop(['Unnamed: 0'],axis=1)\n",
    "    #merge historical data with new data\n",
    "    new_historical = pd.merge(left=data3, right=df_work, left_on='Setor', right_on='Setor')\n",
    "    new_historical=new_historical.sort_values(by=[today],ascending=False)\n",
    "    #Save new data as historical data\n",
    "    new_historical.to_csv('../original_files/historical_data_person.csv')\n",
    "    return new_historical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layoff por setor económico- Quantidade de trabalhadores por género em layoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layoff_companies_peopleAmount_byGender():\n",
    "    data2 = pd.read_csv('../original_files/Layoff – Estim. - CAE,Dim,Dist.csv')\n",
    "    #Select the relevant information, deleting the first 8 rows\n",
    "    df=data2.iloc[8:]\n",
    "    #Select the columns that are relevant to work sectors\n",
    "    df_work=df[df.columns[0:6]]\n",
    "    df_work=df_work.iloc[2:23]\n",
    "    #Rename columns for data cleaning\n",
    "    df_work=df_work.drop(['EEs QUE ENTREGARAM DOCUMENTO - COVID19 - Layoff Simplificado'],axis=1)\n",
    "    df_work=df_work.rename(columns={'Unnamed: 1':'Setor','Unnamed: 2':'Nº empresas','Unnamed: 3':'Nº TRABALHADORES','Unnamed: 4':'Feminino','Unnamed: 5':'Masculino'})\n",
    "    #Save new data\n",
    "    df_work.to_csv('../dataframes/df_work.csv')\n",
    "    return df_work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layoff por dimensão da empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organization_dimension():\n",
    "    data5 = pd.read_csv('../original_files/historical_data_company_size.csv')\n",
    "    data5=data5.drop(['Unnamed: 0'],axis=1)\n",
    "    data6 = pd.read_csv('original_files/Layoff – Estim. - CAE,Dim,Dist.csv')\n",
    "    #Create new dataset\n",
    "    df=data6 \n",
    "    # Search for row and column position of desired data\n",
    "    i, c = np.where(df == 'até 10 trabalhadores')\n",
    "    e, d=np.where(df=='>= 250 trabalhadores')\n",
    "    #Create names for data range, for easier identification\n",
    "    row_size=i+2\n",
    "    col1=(c+1)\n",
    "    col2=(d+1)\n",
    "    #Splice the data with the selected rows found on the previous step\n",
    "    df2=df.iloc[int(i):int(row_size),int(c):int(col2)]\n",
    "    #perform some data cleaning\n",
    "    df2=df2.reset_index()\n",
    "    df2=df2.drop('index',1)\n",
    "    df2.columns=df2.iloc[0]\n",
    "    df2=df2.drop([0])\n",
    "    #transpose the data to match the axis of the historical data\n",
    "    df2=df2.transpose()\n",
    "    df2=df2.reset_index()\n",
    "    #import today´s date\n",
    "    today=date.today()\n",
    "    #Rearrange column names\n",
    "    df2.columns=['TOTAL',today]\n",
    "    new_historical = pd.merge(left=data5, right=df2, left_on='TOTAL', right_on='TOTAL')\n",
    "    new_historical=new_historical.sort_values(by=[today],ascending=False)\n",
    "    new_historical.to_csv('../original_files/historical_data_company_size.csv')\n",
    "    return new_historical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layoff por região"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_data():\n",
    "    #Select new dataset\n",
    "    data4 = pd.read_csv('../original_files/Layoff – Estim. - CAE,Dim,Dist.csv')\n",
    "    df=data4\n",
    "    #Drop all set of row and columns that are filled with NaN\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    #Delete the first rows that don't present any important information\n",
    "    df=df.iloc[9:]\n",
    "    #Search for the relevant information that presents the region information, and store the column number\n",
    "    lower=df.columns.get_loc(\"EEs QUE ENTREGARAM DOCUMENTO - COVID19 - Layoff Simplificado.1\")\n",
    "    #Select the upper range of columns with relevant data\n",
    "    upper=lower+3\n",
    "    #filter the data with the selected parameters\n",
    "    df_region=df.iloc[:,lower:upper]\n",
    "    #rename columns and do some data cleaning\n",
    "    df_region=df_region.rename(columns={'EEs QUE ENTREGARAM DOCUMENTO - COVID19 - Layoff Simplificado.1':'Region',\n",
    "                                    'Unnamed: 8':'N° Empresas',\n",
    "                       'Unnamed: 9':'Percentual(%)'})\n",
    "    df_region['Region']=df_region['Region'].replace({'AVEIRO':'Aveiro','BEJA':'Beja','BRAGA':'Braga','BRAGANÇA':'Bragança',\n",
    "                                                 'CASTELO BRANCO':'Castelo Branco','COIMBRA':'Coimbra','ÉVORA':'Évora',\n",
    "                                                 'FARO':'Faro','GUARDA':'Guarda','LEIRIA':'Leiria','LISBOA':'Lisboa',\n",
    "                                                 'PORTALEGRE':'Portalegre','PORTO':'Porto','SANTARÉM':'Santarém',\n",
    "                                                 'SETÚBAL':'Setúbal','VIANA DO CASTELO':'Viana do Castelo',\n",
    "                                                 'VILA REAL':'Vila Real','VISEU':'Viseu','R.A.AÇORES':'R.A.Açores',\n",
    "                                                 'R.A.MADEIRA':'R.A.Madeira'})\n",
    "    df_region['Percentual(%)']=df_region['Percentual(%)']*100\n",
    "    df_region=df_region.reset_index()\n",
    "    df_region=df_region.drop('index',1)\n",
    "    df_region.dropna(axis=0, how='any', inplace=True)\n",
    "    df_region.to_csv(r'../dataframes/organization_region.csv', index = False)\n",
    "    return df_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redução de Actividade TI e MOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redução de atividade por Dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducao_atividade_byday():\n",
    "    \n",
    "    reducao_atividade=pd.read_csv('../original_files/Redução de Actividade TI e MOE.csv')\n",
    "    # Drop rows with no info\n",
    "    reducao_atividade.dropna(how='all',inplace=True)\n",
    "    # Reset indexes\n",
    "    reducao_atividade.reset_index(drop=True, inplace=True)\n",
    "    # Get indexes that separate tables\n",
    "    m = np.concatenate(( [True], reducao_atividade.iloc[:,1:13].isna().all(axis=1), [True] )) \n",
    "    indexes = np.flatnonzero(m[1:] != m[:-1])\n",
    "    # Separate tables\n",
    "    red_byday = reducao_atividade[indexes[0]+2:indexes[1]]\n",
    "    \n",
    "    # Clean byday table \n",
    "    red_byday=red_byday.dropna(axis=1,how='all')\n",
    "    red_byday.columns = ['DATA', 'TI_ParagemTotal', 'TI_Reducao','TI_Total', 'PRO_TI_ParagemTotal', 'PRO_TI_Reducao','PRO_TI_Total', 'MOE_ParagemTotal', 'MOE_Reducao','MOE_Total', 'PRO_MOE_ParagemTotal', 'PRO_MOE_Reducao','PRO_MOE_Total']\n",
    "\n",
    "    # Formatting the date - sometimes some values are already of data type\n",
    "#     date_inds=pd.notnull(pd.to_datetime(red_byday['DATA'],errors='coerce'))\n",
    "#     if date_inds.index[date_inds].size > 0:\n",
    "#         red_byday['DATA'][date_inds]=red_byday['DATA'][date_inds].dt.strftime('%d/%b')\n",
    "#     red_byday['DATA'] = red_byday['DATA'].apply(dateparser.parse,date_formats=['%d/%b'])\n",
    "    red_byday['DATA1']=red_byday['DATA']\n",
    "    red_byday['DATA'] = pd.to_datetime(red_byday['DATA1'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    mask = red_byday['DATA'].isnull()\n",
    "    red_byday.loc[mask, 'DATA'] = red_byday[mask]['DATA1'].apply(dateparser.parse)\n",
    "    red_byday.drop(['DATA1'],axis=1,inplace=True)\n",
    "\n",
    "    # Remove columns that dont have datetype in DATA, for example columns with the TOTAL\n",
    "    red_byday.dropna(subset=['DATA'],inplace=True)\n",
    "    # Reset indexes\n",
    "    red_byday.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save csv\n",
    "    red_byday.to_csv(r'../dataframes/reducao_atividade_porDia.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redução de atividade por Distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducao_atividade_bydistrict():\n",
    "    \n",
    "    reducao_atividade=pd.read_csv('../original_files/Redução de Actividade TI e MOE.csv')\n",
    "    # Drop rows with no info\n",
    "    reducao_atividade.dropna(how='all',inplace=True)\n",
    "    # Reset indexes\n",
    "    reducao_atividade.reset_index(drop=True, inplace=True)\n",
    "    # Get indexes that separate tables\n",
    "    m = np.concatenate(( [True], reducao_atividade.iloc[:,1:13].isna().all(axis=1), [True] )) \n",
    "    indexes = np.flatnonzero(m[1:] != m[:-1])\n",
    "    # Separate tables\n",
    "    red_bydistrict = reducao_atividade[indexes[2]:indexes[3]]\n",
    "    \n",
    "    red_bydistrict_months=red_bydistrict\n",
    "    # Clean bydistrict table \n",
    "    red_bydistrict_months.reset_index(drop=True, inplace=True)\n",
    "    red_bydistrict_months=red_bydistrict_months.set_index(red_bydistrict_months.columns.values[0])\n",
    "    red_bydistrict_months.index.name='Distritos'\n",
    "    # Find columns corresponding to the total counts\n",
    "    checknull_months=red_bydistrict_months.iloc[1].notnull() \n",
    "    checktotal_months=red_bydistrict_months.iloc[1].str.lower()!='total'\n",
    "    checknull_title=red_bydistrict_months.iloc[0].notnull()\n",
    "    # Generate multi level columns\n",
    "    col_indexes=np.nonzero(checknull_title.array)\n",
    "    col_indexes=col_indexes[0]\n",
    "    columns=[]\n",
    "    for j in range(4):\n",
    "        if j<3: \n",
    "            max_range=col_indexes[j+1]\n",
    "        else:\n",
    "            max_range=red_bydistrict_months.shape[1]\n",
    "        for i in range(col_indexes[j],max_range):\n",
    "            columns.append((red_bydistrict_months.iloc[0,col_indexes[j]],red_bydistrict_months.iloc[1,i]))\n",
    "    red_bydistrict_months.columns = pd.MultiIndex.from_tuples(columns,names=['Type', 'Month'])\n",
    "    # Get columns concerning each month\n",
    "    red_bydistrict_months=red_bydistrict_months.iloc[2:, checknull_months.array & checktotal_months.array]\n",
    "    \n",
    "    # Create dataframe with total counts for each district\n",
    "    red_bydistrict_months=red_bydistrict_months.astype('int64')\n",
    "    red_bydistrict_total=red_bydistrict_months.groupby(level=['Type'],axis=1).sum()\n",
    "    red_bydistrict_total.columns=['PRO_MOE','PRO_TI','MOE','TI']\n",
    "\n",
    "    # Save csvs (per month and total)\n",
    "    red_bydistrict_months.to_csv(r'../dataframes/reducao_atividade_porDistrito_porMes.csv')\n",
    "    red_bydistrict_total.to_csv(r'../dataframes/reducao_atividade_porDistrito_Total.csv')\n",
    "    \n",
    "    ## APPEND TO HISTORICAL DATA\n",
    "    # Transform columns into indexes\n",
    "    df_aux=red_bydistrict_total.stack().to_frame()\n",
    "    # Change columns to the file's date\n",
    "    df_aux.columns=[date.today()]\n",
    "\n",
    "    # Import dataframe with historical data\n",
    "    red_bydistrict_historical=pd.read_csv('../dataframes/reducao_atividade_bydistrict_historicalData.csv',index_col=[0,1])\n",
    "    # Append to dataframe with historical data for reducao de atividade por distrito\n",
    "    red_bydistrict_historical=pd.concat([red_bydistrict_historical,df_aux],axis=1,join='outer')\n",
    "    red_bydistrict_historical.to_csv('../dataframes/reducao_atividade_bydistrict_historicalData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redução de atividade por Sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducao_atividade_bysex():\n",
    "    \n",
    "    reducao_atividade=pd.read_csv('../original_files/Redução de Actividade TI e MOE.csv')\n",
    "    # Drop rows with no info\n",
    "    reducao_atividade.dropna(how='all',inplace=True)\n",
    "    # Reset indexes\n",
    "    reducao_atividade.reset_index(drop=True, inplace=True)\n",
    "    # Get indexes that separate tables\n",
    "    m = np.concatenate(( [True], reducao_atividade.iloc[:,1:13].isna().all(axis=1), [True] )) \n",
    "    indexes = np.flatnonzero(m[1:] != m[:-1])\n",
    "    # Separate tables\n",
    "    red_bysex = reducao_atividade[indexes[4]+2:indexes[5]]\n",
    "    \n",
    "    # Clean bysex table \n",
    "    red_bysex=red_bysex.dropna(axis=1)\n",
    "    red_bysex.columns=['TipoPedido','Total','Feminino','Masculino']\n",
    "    if len(red_bysex.iloc[:,0])==3:\n",
    "        red_bysex.iloc[:,0]=['Trabalhador Independente','Prorrogação Trabalhador Independente','Membro Orgão Estatutário']\n",
    "    elif len(red_bysex.iloc[:,0])==4:\n",
    "        red_bysex.iloc[:,0]=['Trabalhador Independente','Prorrogação Trabalhador Independente','Membro Orgão Estatutário','Prorrogação Membro Orgão Estatutário']\n",
    "    red_bysex.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save csv\n",
    "    red_bysex.to_csv(r'../dataframes/reducao_atividade_porSexo.csv', index = False)\n",
    "    \n",
    "    ## APPEND TO HISTORICAL DATA\n",
    "    # Transform columns into indexes\n",
    "    red_bysex_new=red_bysex.set_index('TipoPedido').stack().to_frame()\n",
    "    # Change columns to the file's date\n",
    "    red_bysex_new.columns=[date.today()]\n",
    "\n",
    "    # Import dataframe with historical data\n",
    "    red_bysex_historical=pd.read_csv('../dataframes/reducao_atividade_bysex_historicalData.csv',index_col=[0,1])\n",
    "    # # Append to dataframe with historical data for reducao de atividade por sexo\n",
    "    red_bysex_historical=pd.concat([red_bysex_historical,red_bysex_new],axis=1,join='outer')\n",
    "    red_bysex_historical.to_csv('../dataframes/reducao_atividade_bysex_historicalData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despedimentos coletivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def despedimentos_coletivos():\n",
    "    despedimentos=pd.read_csv('../original_files/Despedimentos coletivos.csv')\n",
    "    # Drop first column which is empty\n",
    "    despedimentos=despedimentos.drop(despedimentos.columns[0],axis=1)\n",
    "    # Drop rows with nan\n",
    "    despedimentos.dropna(inplace=True)\n",
    "    despedimentos.reset_index(drop=True, inplace=True)\n",
    "    # Rename columns\n",
    "    despedimentos.columns = ['DATA', 'COLETIVOS_TOTAL', 'COLETIVOS_MICRO','TRABALHADORES_TOTAL', 'TRABALHADORES_MICRO']\n",
    "    # Save csv\n",
    "    despedimentos.to_csv(r'../dataframes/despedimentos_coletivos.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
